Endurance Lab AI üèÜ
Asistente virtual para entrenamiento de resistencia ‚Äî ciclismo ¬∑ running ¬∑ triatl√≥n

Autor: Juan Felipe Cardona Arango
Fecha: Mayo 2025
Cursos: Procesamiento de Lenguaje Natural y Experiencias en Inteligencia de Negocios
Docentes: Juan David Mart√≠nez Vargas ¬∑ Ana Mar√≠a L√≥pez Moreno ¬∑ Edwin Nelson Montoya M√∫nera
√çtem: Proyecto final de ambos cursos

------------------------------------------------------------

üìñ Descripci√≥n

Este repositorio contiene la implementaci√≥n y evaluaci√≥n de Endurance Lab AI, un asistente virtual experto en entrenamiento de resistencia. Incluye:

- Personalizaci√≥n del dominio y los prompts
- Conjunto de pruebas "eval_dataset.csv"
- Evaluaci√≥n autom√°tica con LangChain y MLflow
- Dashboard interactivo de m√©tricas
- Reflexiones y an√°lisis comparativo
- Criterio adicional de Claridad

------------------------------------------------------------

üóÇÔ∏è Tabla de Contenidos

1. Parte 1: Personalizaci√≥n
2. Parte 2: Evaluaci√≥n Autom√°tica
3. Parte 3: Reto Investigador
4. Parte 4: Dashboard
5. Parte 5: Presentaci√≥n y Reflexi√≥n
6. Bonus: Claridad
7. C√≥mo Ejecutar
8. Licencia

------------------------------------------------------------

Parte 1: Personalizaci√≥n

1. Dominio
Asistente especializado en entrenamiento de resistencia: ciclismo, triatl√≥n, nataci√≥n y running.

2. Documentos internos utilizados:
- planes_entrenamiento.pdf
- historiales_rendimiento.pdf
- guias_nutricion.pdf
- revisiones_bibliograficas.pdf

3. Prompts utilizados:

Prompt principal:
- SISTEMA: Eres Endurance Lab AI, un asistente virtual experto...
- RESPONSABILIDADES: Validar contexto, solicitar datos faltantes, responder con citas.
- FORMATO: USUARIO {question}, CONTEXTO {context}, ASISTENTE ...

Prompt secundario:
- SISTEMA: Responde solo con informaci√≥n interna. S√© breve y directo.

4. Conjunto de pruebas:
- Archivo: eval_dataset.csv
- Casos t√≠picos y extremos

------------------------------------------------------------

Parte 2: Evaluaci√≥n Autom√°tica

- Script: run_eval.py
- Evaluador: LangChain (correctness, relevance, coherence, toxicity, harmfulness)
- Registro: MLflow

------------------------------------------------------------

Parte 3: Reto Investigador

Se agregaron m√©tricas avanzadas:
- *_score: valor num√©rico
- *_reasoning: justificaci√≥n textual

------------------------------------------------------------

Parte 4: Dashboard

- dashboard.py: visualizaci√≥n por criterio
- app/main_interface.py: selector de experimentos y gr√°ficos interactivos

------------------------------------------------------------

Parte 5: Presentaci√≥n y Reflexi√≥n

Resultados por configuraci√≥n:

| Chunk Size | Prompt  | Correctness | Relevance | Coherence | Toxicity | Harmfulness |
|------------|---------|-------------|-----------|-----------|----------|-------------|
| 512        | A       | 0.87        | 0.85      | 0.82      | 0.00     | 0.00        |
| 256        | B       | 0.92        | 0.90      | 0.88      | 0.00     | 0.00        |

Observaciones:
- Mejor configuraci√≥n: Chunk 256 + Prompt B
- Fallos detectados: contexto incompleto, formato inconsistente, citas ausentes

------------------------------------------------------------

Bonus: Claridad

Evaluaci√≥n de la claridad para usuarios deportistas (fluidez, jerga, estructura)

C√≥digo base (LangChain):
- clarity_score: puntuaci√≥n
- clarity_reasoning: justificaci√≥n

------------------------------------------------------------

üõ† C√≥mo Ejecutar

1. Instalar dependencias:
   pip install -r requirements.txt

2. Ejecutar evaluaci√≥n:
   python run_eval.py --dataset eval_dataset.csv

3. Lanzar aplicaci√≥n Streamlit:
   streamlit run app/main_interface.py

Accede desde tu navegador en http://localhost:8501

------------------------------------------------------------

üìÑ Licencia

Este proyecto est√° licenciado bajo los t√©rminos de la Licencia MIT.
Consulta el archivo LICENSE para m√°s detalles.

