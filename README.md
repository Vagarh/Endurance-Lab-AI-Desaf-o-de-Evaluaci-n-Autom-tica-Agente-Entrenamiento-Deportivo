````markdown
<p align="center">
  <img src="assets/banner.png" alt="EnduranceÂ LabÂ AI banner" style="max-width:100%;height:auto;border-radius:6px;">
</p>

<p align="center">
  <img src="assets/logo.png" alt="EnduranceÂ LabÂ AI logo" width="160">
</p>

<h1 align="center">EnduranceÂ LabÂ AIÂ ğŸ†</h1>
<p align="center">Asistente virtual para <strong>entrenamiento de resistencia</strong> â€” ciclismo Â· running Â· triatlÃ³n.</p>
<p align="center">
  <a href="LICENSE"><img src="https://img.shields.io/github/license/Vagarh/Endurance-Lab-AI-Desaf-o-de-Evaluaci-n-Autom-tica-Agente-Entrenamiento-Deportivo?style=flat-square" alt="MITÂ License"></a>
  <img src="https://img.shields.io/badge/Streamlit-1.34+-brightgreen?style=flat-square" alt="Streamlit">
  <img src="https://img.shields.io/badge/LangChain-0.2+-violet?style=flat-square" alt="LangChain">
  <img src="https://img.shields.io/badge/MLflow-2.12+-blue?style=flat-square" alt="MLflow">
</p>

---

> **Autor:** JuanÂ FelipeÂ CardonaÂ Arango Â Â Â·Â Â  **Fecha:** mayoÂ 2025  
> **Cursos:** *Procesamiento de Lenguaje Natural*Â yÂ *Experiencias en Inteligencia de Negocios*  
> **Docentes:** JuanÂ DavidÂ MartÃ­nezÂ VargasÂ Â· AnaÂ MarÃ­aÂ LÃ³pezÂ MorenoÂ Â· EdwinÂ NelsonÂ MontoyaÂ MÃºnera  
> **Ãtem:** Proyecto final de ambos cursos

---

## ğŸ“–Â DescripciÃ³n

Este repositorio contiene la implementaciÃ³n y evaluaciÃ³n de **EnduranceÂ LabÂ AI**, un asistente virtual experto en entrenamiento de resistencia. Incluye:

- âš™ï¸Â **PersonalizaciÃ³n** de dominio y prompts.  
- âœ…Â **Conjunto de pruebas** `eval_dataset.csv`.  
- ğŸ¤–Â **EvaluaciÃ³n automÃ¡tica** con LangChain + MLflow.  
- ğŸ“ŠÂ **Dashboard interactivo** de mÃ©tricas.  
- ğŸ“šÂ **Reflexiones** y anÃ¡lisis comparativo.  
- âœ¨Â Criterio adicional de **Claridad**.

---

## ğŸ—‚ï¸Â Tabla de Contenidos

1. [ParteÂ 1Â Â·Â PersonalizaciÃ³n](#parte-1-personalizaciÃ³n)  
2. [ParteÂ 2Â Â·Â EvaluaciÃ³n AutomÃ¡tica](#parte-2-evaluaciÃ³n-automÃ¡tica)  
3. [ParteÂ 3Â Â·Â Reto Investigador](#parte-3-reto-investigador)  
4. [ParteÂ 4Â Â·Â Dashboard](#parte-4-dashboard)  
5. [ParteÂ 5Â Â·Â PresentaciÃ³n y ReflexiÃ³n](#parte-5-presentaciÃ³n-y-reflexiÃ³n)  
6. [ğŸš€Â BonusÂ Â·Â Criterio â€œClaridadâ€](#-bonus-criterio-claridad)  
7. [ğŸ› ï¸Â CÃ³mo Ejecutar](#ï¸-cÃ³mo-ejecutar)  
8. [ğŸ“„Â Licencia](#-licencia)

---

## ParteÂ 1Â Â·Â PersonalizaciÃ³n

### 1. DominioÂ ğŸ…
* Entrenamiento deportivo de resistencia.  
* Especialidades: **ciclismo, triatlÃ³n, nataciÃ³n, running**.

### 2. Documentos InternosÂ ğŸ“‚
Se sustituyeron los PDFs base por:

| Tipo de documento          | Archivo                         |
|----------------------------|---------------------------------|
| Planes de entrenamiento    | `planes_entrenamiento.pdf`      |
| Historiales de rendimiento | `historiales_rendimiento.pdf`   |
| GuÃ­as de nutriciÃ³n         | `guias_nutricion.pdf`           |
| Revisiones bibliogrÃ¡ficas  | `revisiones_bibliograficas.pdf` |

### 3. PromptsÂ ğŸ“
<details>
<summary>Prompt principal</summary>

```text
SISTEMA:
Eres EnduranceÂ LabÂ AI, un asistente virtual experto en entrenamiento de resistenciaâ€¦
RESPONSABILIDADES:
1. Validar contextoâ€¦
2. Solicitar datos faltantesâ€¦
3. Responder con citaâ€¦
FORMATO:
- USUARIO: {question}
- CONTEXTO: {context}
- ASISTENTE: â€¦
````

</details>

<details>
<summary>Prompt secundario (breve y directo)</summary>

```text
SISTEMA:
Eres EnduranceÂ LabÂ AI. Responde solo con informaciÃ³n interna.
- SÃ© breveâ€¦
```

</details>

### 4. Conjunto de pruebasÂ ğŸ§ª

* Archivo: `eval_dataset.csv`
* Cobertura de casos tÃ­picos ğŸ–‰ y extremos ğŸ› ï¸.

---

## ParteÂ 2Â Â·Â EvaluaciÃ³n AutomÃ¡tica

| Recurso       | DescripciÃ³n                                                                                |
| ------------- | ------------------------------------------------------------------------------------------ |
| `run_eval.py` | Script de evaluaciÃ³nÂ âš™ï¸                                                                    |
| **LangChain** | `LabeledCriteriaEvalChain` para *correctness, relevance, coherence, toxicity, harmfulness* |
| **MLflow**    | Registro de mÃ©tricas y artefactosÂ ğŸ“ˆ                                                       |

---

## ParteÂ 3Â Â·Â Reto Investigador

> Se aÃ±adieron criterios avanzados; cada uno guarda mÃ©trica `*_score` y razonamiento `*_reasoning`.

---

## ParteÂ 4Â Â·Â Dashboard

| Archivo                 | Funcionalidad                                                |
| ----------------------- | ------------------------------------------------------------ |
| `dashboard.py`          | GrÃ¡ficos de mÃ©tricas por criterioÂ ğŸ“Š                         |
| `app/main_interface.py` | Selector de experimentos, tabla interactiva, grÃ¡ficas Altair |

---

## ParteÂ 5Â Â·Â PresentaciÃ³n y ReflexiÃ³n

| ConfiguraciÃ³n          | Correctness | Relevance | Coherence | Toxicity | Harmfulness |
| ---------------------- | :---------: | :-------: | :-------: | :------: | :---------: |
| ChunkÂ =Â 512 Â· PromptÂ A |   **0.87**  |    0.85   |    0.82   |   0.00   |     0.00    |
| ChunkÂ =Â 256 Â· PromptÂ B |   **0.92**  |    0.90   |    0.88   |   0.00   |     0.00    |

* ğŸ¯ **Mejor**: *chunkÂ 256 + PromptÂ B*
* âš ï¸Â Fallos: contexto incompleto, citas omitidas, formateo inconsistente.
* ğŸ§©Â Toxicidad/Incoherencia: **0Â %** toxicidad; leves incoherencias en chunks grandes.

---

## ğŸš€Â BonusÂ Â·Â Criterio â€œClaridadâ€

Mide la facilidad de comprensiÃ³n para deportistas (fluidez, estructura, jerga mÃ­nima).

```python
from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain

criteria = [
    {"name": "clarity", "description": "Â¿La respuesta es clara y fÃ¡cil de entender para un deportista?"},
]

eval_chain = LabeledCriteriaEvalChain.from_criteria(
    llm=llm,
    criteria=criteria,
    input_key="response",
    prediction_key="score",
)
```

MÃ©trica en MLflow: **`clarity_score`** + artifact **`clarity_reasoning`**.

---

## ğŸ› ï¸Â CÃ³mo Ejecutar

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Ejecutar evaluaciÃ³n automÃ¡tica
python run_eval.py --dataset eval_dataset.csv

# 3. Iniciar la app Streamlit
streamlit run app/main_interface.py
```

*La app estarÃ¡ disponible en [http://localhost:8501](http://localhost:8501).*

---

## ğŸ“„Â Licencia

Este proyecto estÃ¡ bajo la **LicenciaÂ MIT**. Consulta el archivo [`LICENSE`](LICENSE) para mÃ¡s detalles.

```
```
