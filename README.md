# Endurance Lab AI ‚Äî Desaf√≠o de Evaluaci√≥n Autom√°tica Agente Entrenamiento Deportivo.

Autor: Juan Felipe Cardona Arango  
Fecha: Mayo 2025
Cursos: Procesamiento de Lenguaje Natural y Experiencias en Inteligencia de Negocios
Docentes: Juan David Martinez Vargas, Ana Maria Lopez Moreno, Edwin Nelson Montoya Munera.
Item: Proyecto final de cursos.
---

üìñ Descripci√≥n

Este repositorio contiene la implementaci√≥n y evaluaci√≥n de Endurance Lab AI, un asistente virtual experto en entrenamiento de resistencia (ciclismo, running, triatl√≥n). Incluye:

- Personalizaci√≥n del dominio y prompts
- Conjunto de pruebas (eval_dataset.csv)
- Integraci√≥n de evaluaci√≥n autom√°tica con LangChain y MLflow
- Dashboard interactivo de m√©tricas
- Reflexiones y an√°lisis comparativo
- Criterio adicional de ‚ÄúClaridad‚Äù

---

üìë Tabla de Contenidos

1. Parte 1: Personalizaci√≥n  
2. Parte 2: Evaluaci√≥n Autom√°tica  
3. Parte 3: Reto Investigador  
4. Parte 4: Dashboard  
5. Parte 5: Presentaci√≥n y Reflexi√≥n  
6. üöÄ Bonus: Criterio ‚ÄúClaridad‚Äù  
7. üõ†Ô∏è C√≥mo ejecutar  
8. üìÑ Licencia  

---

Parte 1: Personalizaci√≥n

1. Dominio
- Dominio de entrenamiento deportivo de resistencia  
- Especialidades: ciclismo, triatl√≥n, nataci√≥n, running  

2. Documentos Internos
Se reemplazaron los PDFs base por:

- planes_entrenamiento.pdf  
- historiales_rendimiento.pdf  
- guias_nutricion.pdf  
- revisiones_bibliograficas.pdf  

3. Prompts

Prompt Principal
SISTEMA:
Eres Endurance Lab AI, un asistente virtual experto en entrenamiento de resistencia (ciclismo, running, triatl√≥n).
Utiliza solo estos documentos internos:
- Planes de entrenamiento
- Historiales de rendimiento
- Gu√≠as de nutrici√≥n
- Revisiones bibliogr√°ficas

RESPONSABILIDADES:
1. Validar contexto (plan actual, objetivos, nivel f√≠sico, restricciones, disponibilidad).
2. Solicitar datos faltantes con preguntas breves.
3. Responder con cita de fuente interna; no hacer conjeturas.
4. Tono t√©cnico, emp√°tico y motivador.

FORMATO:
- USUARIO: ‚Äú{question}‚Äù
- CONTEXTO: ‚Äú{context}‚Äù
- ASISTENTE:
  1. Comprobaci√≥n de contexto
  2. Petici√≥n de informaci√≥n (si aplica)
  3. Respuesta (con cita)

Prompt Secundario (Breve y Directo)
SISTEMA:
Eres Endurance Lab AI. Responde solo con informaci√≥n interna.
- S√© breve y directo.
- Si falta informaci√≥n, responde: ‚ÄúNo tengo informaci√≥n suficiente‚Äù.

FORMATO:
- Pregunta: {question}
- Contexto: {context}
- Respuesta:

4. Conjunto de Pruebas
- Archivo: eval_dataset.csv
- Cobertura de casos t√≠picos y extremos.

---

Parte 2: Evaluaci√≥n Autom√°tica

- Script de evaluaci√≥n: run_eval.py
- Integraci√≥n con LangChain:
  - LabeledCriteriaEvalChain para criterios:
    - correctness
    - relevance
    - coherence
    - toxicity
    - harmfulness
- Reporte de m√©tricas en MLflow.

---

Parte 3: Reto Investigador

- A√±adidos criterios avanzados usando LabeledCriteriaEvalChain
- Cada criterio:
  - M√©trica (*_score)
  - Razonamiento opcional como artifact (*_reasoning)

---

Parte 4: Dashboard

- Archivos modificados:
  - dashboard.py
  - main_interface.py
- Funcionalidades:
  - Gr√°ficos de m√©tricas por criterio
  - Selector para comparar criterios
  - Visualizaci√≥n de razonamientos opcionales

---

Parte 5: Presentaci√≥n y Reflexi√≥n

Comparaci√≥n de configuraciones:

Configuraci√≥n             | Correctness | Relevance | Coherence | Toxicity | Harmfulness
---------------------------|-----------:|----------:|----------:|---------:|------------:
Chunk=512 + Prompt A      |        0.87 |      0.85 |      0.82 |     0.00 |        0.00
Chunk=256 + Prompt B      |        0.92 |      0.90 |      0.88 |     0.00 |        0.00

- Mejor configuraci√≥n: chunk=256 + Prompt B
- Fallos detectados: contexto incompleto, citas omitidas, formateo inconsistente.
- Toxicidad/Incoherencia: cero toxicidad; algunas incoherencias en chunks grandes.

---

üöÄ Bonus: Criterio ‚ÄúClaridad‚Äù

Descripci√≥n: mide la facilidad de comprensi√≥n para deportistas (fluidez, estructura, jerga m√≠nima).

C√≥digo de implementaci√≥n con LabeledCriteriaEvalChain:
```python
from langchain.evaluation.criteria.eval_chain import LabeledCriteriaEvalChain

criteria = [
    {"name": "clarity", "description": "¬øLa respuesta es clara y f√°cil de entender para un deportista?"},
    # dem√°s criterios...
]

eval_chain = LabeledCriteriaEvalChain.from_criteria(
    llm=llm,
    criteria=criteria,
    input_key="response",
    prediction_key="score"
)
```

- M√©trica en MLflow: clarity_score
- Artifact: clarity_reasoning

---

üõ†Ô∏è C√≥mo ejecutar

1. Instalar dependencias
   pip install -r requirements.txt
2. Ejecutar evaluaci√≥n
   python run_eval.py --dataset eval_dataset.csv
3. Iniciar dashboard
   python dashboard.py

---

üìÑ Licencia

Este proyecto est√° bajo la licencia MIT. Consulta LICENSE para m√°s detalles.
